# course-optimization

This project focuses on algorithmic analysisâ€”specifically, the application of optimization in deep learning. We replicated six optimizers, ranging from the latest NeurIPS 2020 Spotlight AdaBelief optimizer to the classic SGD and Adam optimizers, and analyzed their convergence speed, stability, saddle points, and other aspects. We used the YChat dataset to build a linear regression model to explore the relationship between various factors and the price of YChat. Our approach offers a fresh perspective on optimizer research in deep learning and the selection of YChat.
